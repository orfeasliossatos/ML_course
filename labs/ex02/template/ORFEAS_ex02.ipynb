{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MSE(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    \n",
    "    return np.sum((y - tx @ w) ** 2) / (2 * y.shape[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MAE(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    \n",
    "    return np.sum(np.abs(y - tx @ w)) / y.shape[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "+ Each column of $\\tilde{X}$ represents a feature.\n",
    "+ Each row of $\\tilde{X}$ represents a datum.\n",
    "+ The 1s in $\\tilde{X}$ add a bias term into matrix math.\n",
    "+ The shape of $y$ would be $3 \\times 1$, the shape of $\\tilde{X}$ would be $3 \\times 3$. Therefore, $\\tilde{X}_{32}$ would represent the height of the third person.\n",
    "+ The helper function `load_data` uses `np.genfromtxt` to load csv data.\n",
    "+ The sizes make sense. \n",
    "#### Proofs.\n",
    "\n",
    "1. Let $e=y-\\tilde{X}w$ be a column vector of errors. Call its $n$'th component $e_n=y_n - \\tilde{x}_n w$, where $\\tilde{x}_n$ is the $n$'th row of $\\tilde{X}$, $w$ is a column vector of weights, and $y_n$ is the $n$'th component of $y$. Then $\\mathcal{L}(w)=\\frac{1}{2N}\\sum_{n=1}^{N}(y_n - \\tilde{x}_n w)^2=\\frac{1}{2N}\\sum_{n=1}^{N}e_n^2=\\frac{1}{2N}e^Te$.\n",
    "2. (Below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:\t 72.29392200210518\n",
      "Mean squared error:\t 2694.4833658870843\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute error:\\t\", compute_loss_MAE(y, tx, [1, 2]))\n",
    "print(\"Mean squared error:\\t\", compute_loss_MSE(y, tx, [1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            losses[i, j] = compute_loss_MSE(y, tx, np.array([w0, w1]))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=18.793541019523236, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.271 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "+ This estimate looks slightly off. The resolution of the grid search is not large enough to get close to the optimal $(w_0^*, w_1^*)$\n",
    "+ The MSE plot is not smooth because it is interpolated from a low resolution grid search.\n",
    "+ The fit is much better for 2500 weight pairs sampled.\n",
    "+ You are more likely to find an accurate fit with a fine grid, but the search is more costly. Furthermore, a course grid could by perfect chance hit the minimum while a slightly finer one does not\n",
    "+ The computational cost increases as a square of the number of intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_MSE(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    gradient = - (e @ tx) / N\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "+ Test $y=[1,2,3]^T$, $\\tilde{X}=\\begin{bmatrix}1 & 2\\\\ 1 & 2\\\\ 1 & 2\\end{bmatrix}$, $w=[1, 2]^T$ yields $\\nabla\\mathcal{L}(w)=[3,6]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 6.])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_y = np.array([1, 2, 3])\n",
    "m_tx = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "m_w = np.array([1, 2])\n",
    "compute_gradient_MSE(m_y, m_tx, m_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Compute gradients for $w_0=100, w_1=20$ and $w_0=50, w_1=10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757]\n",
      "[-23.293922    -3.47971243]\n"
     ]
    }
   ],
   "source": [
    "print(compute_gradient_MSE(y, tx, [100, 20]))\n",
    "print(compute_gradient_MSE(y, tx, [50, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The gradients point in the direction of steepest ascent. If the norm is larger, then we are generally further away from the minimum given that the function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        g = compute_gradient_MSE(y, tx, w)\n",
    "        loss = compute_loss_MSE(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * g\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=18773.6576694708, w0=73.2939220021056, w1=13.479712434988727\n",
      "GD iter. 1/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 2/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 3/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 4/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 5/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 6/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 7/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 8/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 9/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 10/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 11/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 12/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 13/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 14/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 15/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 16/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 17/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 19/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 20/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 25/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.051 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([-100, 100])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ When $\\gamma = 0.001$, convergence is very slow. When $\\gamma = 1$, convergence is instant, so it doesn't matter what values you put for `w_initial`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab080ab3185f44f4aefa3f8ae2b3b0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    N = y.shape\n",
    "    e = y - tx @ w\n",
    "    return - (e @ tx) / N\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_MSE(y, tx, w)\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * g\n",
    "\n",
    "        \n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.2367127591674, w0=5.128640039746891, w1=-6.964275108135858\n",
      "SGD iter. 1/49: loss=2547.617033704456, w0=11.691255676870457, w1=-10.4422765115824\n",
      "SGD iter. 2/49: loss=2198.9609146378757, w0=19.62751182793601, w1=-3.3958338523908687\n",
      "SGD iter. 3/49: loss=1597.8197096086637, w0=24.304805680919, w1=-4.9899317206659095\n",
      "SGD iter. 4/49: loss=1385.9165244524436, w0=32.68222032039077, w1=11.827954156670073\n",
      "SGD iter. 5/49: loss=841.4051973161099, w0=35.60044906870335, w1=9.719365437309003\n",
      "SGD iter. 6/49: loss=732.8549435308585, w0=38.864282283965935, w1=12.748701430166966\n",
      "SGD iter. 7/49: loss=608.353121973851, w0=43.230636656852965, w1=11.76099808254741\n",
      "SGD iter. 8/49: loss=468.7634402565024, w0=44.77566940093758, w1=13.73275976211907\n",
      "SGD iter. 9/49: loss=422.0632700557148, w0=48.31928382601656, w1=14.384530360014253\n",
      "SGD iter. 10/49: loss=327.6615116208247, w0=51.38580679801366, w1=10.514909092294895\n",
      "SGD iter. 11/49: loss=259.7636731971277, w0=53.78883514328477, w1=11.459921637087605\n",
      "SGD iter. 12/49: loss=207.64987198753752, w0=55.88828713734703, w1=14.18395734173429\n",
      "SGD iter. 13/49: loss=167.11193083581014, w0=58.02658757216847, w1=16.654211065636993\n",
      "SGD iter. 14/49: loss=136.97035894458793, w0=60.6378383040085, w1=13.628656901104174\n",
      "SGD iter. 15/49: loss=95.48520738243698, w0=62.43613855556735, w1=14.644504982423047\n",
      "SGD iter. 16/49: loss=75.0099893940639, w0=63.137795374317314, w1=15.626395333755646\n",
      "SGD iter. 17/49: loss=69.26346564258888, w0=64.65656974735599, w1=14.025238161207014\n",
      "SGD iter. 18/49: loss=52.83661401412279, w0=65.01880695787028, w1=14.323362014202631\n",
      "SGD iter. 19/49: loss=49.98052467274448, w0=65.64790933823171, w1=15.240743882832154\n",
      "SGD iter. 20/49: loss=46.16725857703239, w0=65.68163669801318, w1=15.269064214260252\n",
      "SGD iter. 21/49: loss=45.96022153926752, w0=66.71436609965977, w1=16.25408410356099\n",
      "SGD iter. 22/49: loss=40.87973488321894, w0=68.58996105238319, w1=14.639342023370345\n",
      "SGD iter. 23/49: loss=27.12188256820885, w0=68.78284471335202, w1=14.350075236743711\n",
      "SGD iter. 24/49: loss=25.939562724720844, w0=69.41177288327088, w1=14.430038376661933\n",
      "SGD iter. 25/49: loss=23.372988456970617, w0=69.91289418566006, w1=14.023534025737698\n",
      "SGD iter. 26/49: loss=21.249433377899443, w0=69.29356504172257, w1=13.075946268179754\n",
      "SGD iter. 27/49: loss=23.46882933280016, w0=70.01126118468666, w1=14.026679251422047\n",
      "SGD iter. 28/49: loss=20.92340523907627, w0=69.46208419245254, w1=13.852826485729627\n",
      "SGD iter. 29/49: loss=22.796985416001245, w0=70.0407001922151, w1=14.204251909524142\n",
      "SGD iter. 30/49: loss=20.940092666081483, w0=71.08519317068203, w1=14.217464009910682\n",
      "SGD iter. 31/49: loss=18.097268087359033, w0=71.22132428695893, w1=14.123679881865959\n",
      "SGD iter. 32/49: loss=17.74106554956273, w0=71.69817214845179, w1=14.797845826917783\n",
      "SGD iter. 33/49: loss=17.527834486005602, w0=72.45251360552858, w1=13.895332512609652\n",
      "SGD iter. 34/49: loss=15.82624193820489, w0=73.1777510592484, w1=12.729138021100225\n",
      "SGD iter. 35/49: loss=15.674316688203794, w0=74.1100515011959, w1=12.75402125919588\n",
      "SGD iter. 36/49: loss=15.982235389784462, w0=74.28888234068565, w1=13.073899144299766\n",
      "SGD iter. 37/49: loss=15.963203119953501, w0=73.19480094723988, w1=11.262037152681234\n",
      "SGD iter. 38/49: loss=17.84984218946773, w0=71.99934521593472, w1=11.491239274913774\n",
      "SGD iter. 39/49: loss=18.200865150644997, w0=72.12439332324664, w1=11.322541963134533\n",
      "SGD iter. 40/49: loss=18.39647875648623, w0=71.98728053972815, w1=11.224135331403364\n",
      "SGD iter. 41/49: loss=18.7833578595408, w0=72.96881542013577, w1=11.48200102265005\n",
      "SGD iter. 42/49: loss=17.434160457144056, w0=72.59453081353664, w1=11.672554764351036\n",
      "SGD iter. 43/49: loss=17.26337130942597, w0=73.36599206243588, w1=12.422308978008546\n",
      "SGD iter. 44/49: loss=15.947535951044594, w0=73.5335653370903, w1=12.216117490956341\n",
      "SGD iter. 45/49: loss=16.2129384241233, w0=73.56380265144237, w1=12.179784437183121\n",
      "SGD iter. 46/49: loss=16.267212051012596, w0=72.57763736894655, w1=11.934715877771634\n",
      "SGD iter. 47/49: loss=16.83592688758583, w0=72.36788527192782, w1=12.101627427047493\n",
      "SGD iter. 48/49: loss=16.764219026204835, w0=71.57855116092296, w1=12.122799069348117\n",
      "SGD iter. 49/49: loss=17.77774337114602, w0=71.21774775433012, w1=12.40004953125812\n",
      "SGD: execution time=0.134 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51063680f66141d78d1a64660d6bbe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=1, min=1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.2821247015965, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260336, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260336, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260336, w0=74.06780585492449, w1=11.034894865988822\n",
      "GD iter. 26/49: loss=65.93073010260336, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.036 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcbed056b614f7d8db73a8780d35fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    \n",
    "    N = y.shape\n",
    "    del_h = np.sign(y - tx @ w)\n",
    "    del_q = -tx\n",
    "    \n",
    "    return np.sum(del_h[:, np.newaxis] * del_q, axis = 0) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "        g = compute_subgradient_mae(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * g\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657144, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512215, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605635, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.42321888875611, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.495041794646427, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313202, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.72808432666813, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158964, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221257, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751496, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.705279728377, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269658, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790257, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993146, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022584, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517473, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643114, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.3130522468713846, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024387, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098433, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.311505047641534, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920624, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306399, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438473, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161021, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318958, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690679, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.310684480220337, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413225, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729927, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.310613573874789, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629318, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326743, w1=15.970593411609551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749847, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.31057969173614, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701808, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481999, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.31057869984858, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814244, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920505, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976984, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.3106225210673275, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.3106260999443435, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237008, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668312, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383831, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555702, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254566, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862546, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903372, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724173, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.31057945788459, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.310582363536381, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.31057952032574, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333343, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.310626664479185, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.3105794901438035, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979162, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064514, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652994, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.3105756696734066, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.31057658755788, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.310626270381019, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705162, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326827, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259014, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.310624523551678, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633672, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.434 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc1283fd29c484489b0565633f8996c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            g = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * g\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=73.06780585492638, w0=1.3, w1=0.9503643722206012\n",
      "SubSGD iter. 1/499: loss=72.76780585492638, w0=1.6, w1=0.9352111192567317\n",
      "SubSGD iter. 2/499: loss=72.46780585492638, w0=1.9000000000000001, w1=0.8989021298833373\n",
      "SubSGD iter. 3/499: loss=72.16780585492639, w0=2.2, w1=0.9316368202095019\n",
      "SubSGD iter. 4/499: loss=71.86780585492637, w0=2.5, w1=0.9134176326835272\n",
      "SubSGD iter. 5/499: loss=71.56780585492636, w0=2.8, w1=0.8879836011230808\n",
      "SubSGD iter. 6/499: loss=71.26780585492638, w0=3.0999999999999996, w1=0.9292667540508348\n",
      "SubSGD iter. 7/499: loss=70.96780585492638, w0=3.3999999999999995, w1=0.9447986442941501\n",
      "SubSGD iter. 8/499: loss=70.66780585492639, w0=3.6999999999999993, w1=0.9220588298888864\n",
      "SubSGD iter. 9/499: loss=70.36780585492637, w0=3.999999999999999, w1=0.9742536305503958\n",
      "SubSGD iter. 10/499: loss=70.06780585492638, w0=4.299999999999999, w1=0.9383083380688809\n",
      "SubSGD iter. 11/499: loss=69.76780585492638, w0=4.599999999999999, w1=0.8772795089395086\n",
      "SubSGD iter. 12/499: loss=69.46780585492638, w0=4.899999999999999, w1=0.8652641008441369\n",
      "SubSGD iter. 13/499: loss=69.16780585492639, w0=5.199999999999998, w1=0.8478583821728349\n",
      "SubSGD iter. 14/499: loss=68.86780585492637, w0=5.499999999999998, w1=0.8433569645117279\n",
      "SubSGD iter. 15/499: loss=68.56780585492638, w0=5.799999999999998, w1=0.8565006738067442\n",
      "SubSGD iter. 16/499: loss=68.26780585492638, w0=6.099999999999998, w1=0.8773062610483764\n",
      "SubSGD iter. 17/499: loss=67.96780585492638, w0=6.399999999999998, w1=0.8795101277587107\n",
      "SubSGD iter. 18/499: loss=67.66780585492639, w0=6.6999999999999975, w1=0.9446221078535542\n",
      "SubSGD iter. 19/499: loss=67.36780585492637, w0=6.999999999999997, w1=0.9849477658019926\n",
      "SubSGD iter. 20/499: loss=67.06780585492638, w0=7.299999999999997, w1=0.9806888639689667\n",
      "SubSGD iter. 21/499: loss=66.76780585492638, w0=7.599999999999997, w1=1.0711394926423796\n",
      "SubSGD iter. 22/499: loss=66.46780585492638, w0=7.899999999999997, w1=1.0967356955220078\n",
      "SubSGD iter. 23/499: loss=66.16780585492639, w0=8.199999999999998, w1=1.0916124826666915\n",
      "SubSGD iter. 24/499: loss=65.86780585492637, w0=8.499999999999998, w1=1.00328955648982\n",
      "SubSGD iter. 25/499: loss=65.56780585492638, w0=8.799999999999999, w1=0.9881510755761959\n",
      "SubSGD iter. 26/499: loss=65.26780585492638, w0=9.1, w1=1.0772044391963247\n",
      "SubSGD iter. 27/499: loss=64.96780585492637, w0=9.4, w1=1.0408972168989903\n",
      "SubSGD iter. 28/499: loss=64.66780585492639, w0=9.700000000000001, w1=1.0154929245995783\n",
      "SubSGD iter. 29/499: loss=64.36780585492637, w0=10.000000000000002, w1=1.0929122614869398\n",
      "SubSGD iter. 30/499: loss=64.06780585492638, w0=10.300000000000002, w1=1.127447324988007\n",
      "SubSGD iter. 31/499: loss=63.76780585492637, w0=10.600000000000003, w1=1.1959111026483789\n",
      "SubSGD iter. 32/499: loss=63.46780585492637, w0=10.900000000000004, w1=1.2766746416488255\n",
      "SubSGD iter. 33/499: loss=63.16780585492637, w0=11.200000000000005, w1=1.2951258220353936\n",
      "SubSGD iter. 34/499: loss=62.867805854926374, w0=11.500000000000005, w1=1.2544428082372772\n",
      "SubSGD iter. 35/499: loss=62.56780585492636, w0=11.800000000000006, w1=1.1717962331695577\n",
      "SubSGD iter. 36/499: loss=62.26780585492637, w0=12.100000000000007, w1=1.1489224500560968\n",
      "SubSGD iter. 37/499: loss=61.96780585492636, w0=12.400000000000007, w1=1.2200118724866504\n",
      "SubSGD iter. 38/499: loss=61.66780585492637, w0=12.700000000000008, w1=1.237177399356469\n",
      "SubSGD iter. 39/499: loss=61.367805854926374, w0=13.000000000000009, w1=1.1551138304070434\n",
      "SubSGD iter. 40/499: loss=61.06780585492636, w0=13.30000000000001, w1=1.1020808587048552\n",
      "SubSGD iter. 41/499: loss=60.76780585492637, w0=13.60000000000001, w1=1.113344024986996\n",
      "SubSGD iter. 42/499: loss=60.46780585492636, w0=13.900000000000011, w1=1.1025036215473605\n",
      "SubSGD iter. 43/499: loss=60.16780585492636, w0=14.200000000000012, w1=1.0203280154354135\n",
      "SubSGD iter. 44/499: loss=59.86780585492636, w0=14.500000000000012, w1=1.0418621917071886\n",
      "SubSGD iter. 45/499: loss=59.56780585492636, w0=14.800000000000013, w1=1.1707001910482617\n",
      "SubSGD iter. 46/499: loss=59.267805854926365, w0=15.100000000000014, w1=1.1935596617400388\n",
      "SubSGD iter. 47/499: loss=58.96780585492636, w0=15.400000000000015, w1=1.226644072563687\n",
      "SubSGD iter. 48/499: loss=58.66780585492637, w0=15.700000000000015, w1=1.124646301661995\n",
      "SubSGD iter. 49/499: loss=58.36780585492636, w0=16.000000000000014, w1=1.1836010334558749\n",
      "SubSGD iter. 50/499: loss=58.06780585492636, w0=16.300000000000015, w1=1.2053873638704489\n",
      "SubSGD iter. 51/499: loss=57.76780585492636, w0=16.600000000000016, w1=1.1446634540609155\n",
      "SubSGD iter. 52/499: loss=57.467805854926354, w0=16.900000000000016, w1=1.1410208363405754\n",
      "SubSGD iter. 53/499: loss=57.16780585492636, w0=17.200000000000017, w1=1.0782619613817472\n",
      "SubSGD iter. 54/499: loss=56.86780585492636, w0=17.500000000000018, w1=1.08223013920142\n",
      "SubSGD iter. 55/499: loss=56.56780585492636, w0=17.80000000000002, w1=1.0900457706489899\n",
      "SubSGD iter. 56/499: loss=56.26780585492636, w0=18.10000000000002, w1=1.1551742024382803\n",
      "SubSGD iter. 57/499: loss=55.967805854926354, w0=18.40000000000002, w1=1.09919039054853\n",
      "SubSGD iter. 58/499: loss=55.66780585492635, w0=18.70000000000002, w1=1.0479026761506525\n",
      "SubSGD iter. 59/499: loss=55.36780585492636, w0=19.00000000000002, w1=1.1380986487013192\n",
      "SubSGD iter. 60/499: loss=55.06780585492635, w0=19.300000000000022, w1=1.183323074346847\n",
      "SubSGD iter. 61/499: loss=54.76780585492636, w0=19.600000000000023, w1=1.1283934690943058\n",
      "SubSGD iter. 62/499: loss=54.46780585492636, w0=19.900000000000023, w1=1.193127198706263\n",
      "SubSGD iter. 63/499: loss=54.16780585492635, w0=20.200000000000024, w1=1.196235747501567\n",
      "SubSGD iter. 64/499: loss=53.86780585492636, w0=20.500000000000025, w1=1.1805132342690174\n",
      "SubSGD iter. 65/499: loss=53.56780585492635, w0=20.800000000000026, w1=1.2493086534261242\n",
      "SubSGD iter. 66/499: loss=53.26780585492636, w0=21.100000000000026, w1=1.251459261630927\n",
      "SubSGD iter. 67/499: loss=52.967805854926354, w0=21.400000000000027, w1=1.1989131806810256\n",
      "SubSGD iter. 68/499: loss=52.66780585492635, w0=21.700000000000028, w1=1.0966175976851178\n",
      "SubSGD iter. 69/499: loss=52.36780585492635, w0=22.00000000000003, w1=1.144955260284244\n",
      "SubSGD iter. 70/499: loss=52.06780585492635, w0=22.30000000000003, w1=1.1287242865332194\n",
      "SubSGD iter. 71/499: loss=51.76780585492636, w0=22.60000000000003, w1=1.15559358656779\n",
      "SubSGD iter. 72/499: loss=51.46780585492635, w0=22.90000000000003, w1=1.174298443824472\n",
      "SubSGD iter. 73/499: loss=51.16780585492634, w0=23.20000000000003, w1=1.2566196814317299\n",
      "SubSGD iter. 74/499: loss=50.86780585492634, w0=23.500000000000032, w1=1.246030644901393\n",
      "SubSGD iter. 75/499: loss=50.56780585492635, w0=23.800000000000033, w1=1.2391604391889925\n",
      "SubSGD iter. 76/499: loss=50.267805854926344, w0=24.100000000000033, w1=1.2673112220849332\n",
      "SubSGD iter. 77/499: loss=49.96780585492635, w0=24.400000000000034, w1=1.2631548395352878\n",
      "SubSGD iter. 78/499: loss=49.667805854926335, w0=24.700000000000035, w1=1.2994791751383397\n",
      "SubSGD iter. 79/499: loss=49.36780585492634, w0=25.000000000000036, w1=1.2798331048038183\n",
      "SubSGD iter. 80/499: loss=49.06780585492634, w0=25.300000000000036, w1=1.2732717496893629\n",
      "SubSGD iter. 81/499: loss=48.76780585492634, w0=25.600000000000037, w1=1.2493765672963415\n",
      "SubSGD iter. 82/499: loss=48.46780585492635, w0=25.900000000000038, w1=1.1754951531892128\n",
      "SubSGD iter. 83/499: loss=48.167805854926335, w0=26.20000000000004, w1=1.1379623411167017\n",
      "SubSGD iter. 84/499: loss=47.86780585492634, w0=26.50000000000004, w1=1.2539221470607254\n",
      "SubSGD iter. 85/499: loss=47.56780585492633, w0=26.80000000000004, w1=1.2119931180685934\n",
      "SubSGD iter. 86/499: loss=47.26780585492634, w0=27.10000000000004, w1=1.183364202105686\n",
      "SubSGD iter. 87/499: loss=46.96780585492633, w0=27.40000000000004, w1=1.1913772929668707\n",
      "SubSGD iter. 88/499: loss=46.667805854926335, w0=27.700000000000042, w1=1.2318537314988547\n",
      "SubSGD iter. 89/499: loss=46.36780585492634, w0=28.000000000000043, w1=1.2676272890106843\n",
      "SubSGD iter. 90/499: loss=46.06780585492633, w0=28.300000000000043, w1=1.1936601745630702\n",
      "SubSGD iter. 91/499: loss=45.76780585492634, w0=28.600000000000044, w1=1.191523275834577\n",
      "SubSGD iter. 92/499: loss=45.46780585492635, w0=28.900000000000045, w1=1.1833241361255225\n",
      "SubSGD iter. 93/499: loss=45.167805854926335, w0=29.200000000000045, w1=1.0759285683104554\n",
      "SubSGD iter. 94/499: loss=44.86780585492633, w0=29.500000000000046, w1=1.0688592794566634\n",
      "SubSGD iter. 95/499: loss=44.56780585492633, w0=29.800000000000047, w1=1.119181039867094\n",
      "SubSGD iter. 96/499: loss=44.26780585492633, w0=30.100000000000048, w1=1.1000250447740447\n",
      "SubSGD iter. 97/499: loss=43.967805854926326, w0=30.40000000000005, w1=1.1810538751401467\n",
      "SubSGD iter. 98/499: loss=43.667805854926335, w0=30.70000000000005, w1=1.1677850493469721\n",
      "SubSGD iter. 99/499: loss=43.367805854926324, w0=31.00000000000005, w1=1.1831583892527322\n",
      "SubSGD iter. 100/499: loss=43.06780585492632, w0=31.30000000000005, w1=1.2298990654105593\n",
      "SubSGD iter. 101/499: loss=42.76780585492633, w0=31.60000000000005, w1=1.2551444865775025\n",
      "SubSGD iter. 102/499: loss=42.467805854926326, w0=31.900000000000052, w1=1.2249801099609672\n",
      "SubSGD iter. 103/499: loss=42.167805854926335, w0=32.20000000000005, w1=1.179467298188075\n",
      "SubSGD iter. 104/499: loss=41.867805854926324, w0=32.50000000000005, w1=1.1339025719483593\n",
      "SubSGD iter. 105/499: loss=41.56780585492633, w0=32.80000000000005, w1=1.1800624801081445\n",
      "SubSGD iter. 106/499: loss=41.26780585492633, w0=33.100000000000044, w1=1.1686960190008142\n",
      "SubSGD iter. 107/499: loss=40.96780585492633, w0=33.40000000000004, w1=1.2157987919403774\n",
      "SubSGD iter. 108/499: loss=40.667805854926335, w0=33.70000000000004, w1=1.243272242647898\n",
      "SubSGD iter. 109/499: loss=40.36780585492634, w0=34.000000000000036, w1=1.1312335993767795\n",
      "SubSGD iter. 110/499: loss=40.06780585492634, w0=34.30000000000003, w1=1.1293004652248173\n",
      "SubSGD iter. 111/499: loss=39.767805854926344, w0=34.60000000000003, w1=1.1676984764698197\n",
      "SubSGD iter. 112/499: loss=39.46780585492635, w0=34.90000000000003, w1=1.1843554189090464\n",
      "SubSGD iter. 113/499: loss=39.16780585492635, w0=35.200000000000024, w1=1.1439418154198429\n",
      "SubSGD iter. 114/499: loss=38.86780585492635, w0=35.50000000000002, w1=1.0831042311438048\n",
      "SubSGD iter. 115/499: loss=38.567805854926355, w0=35.80000000000002, w1=1.0100597759061645\n",
      "SubSGD iter. 116/499: loss=38.26780585492636, w0=36.100000000000016, w1=1.115466233247699\n",
      "SubSGD iter. 117/499: loss=37.96780585492636, w0=36.40000000000001, w1=1.1033359535368537\n",
      "SubSGD iter. 118/499: loss=37.66780585492637, w0=36.70000000000001, w1=1.1292994492972597\n",
      "SubSGD iter. 119/499: loss=37.36780585492637, w0=37.00000000000001, w1=1.0306536531388484\n",
      "SubSGD iter. 120/499: loss=37.06780585492636, w0=37.300000000000004, w1=1.0324457339293207\n",
      "SubSGD iter. 121/499: loss=36.76780585492637, w0=37.6, w1=1.0391626237653577\n",
      "SubSGD iter. 122/499: loss=36.46780585492637, w0=37.9, w1=1.0527292060239597\n",
      "SubSGD iter. 123/499: loss=36.16780585492638, w0=38.199999999999996, w1=0.9928651079141694\n",
      "SubSGD iter. 124/499: loss=35.86780585492638, w0=38.49999999999999, w1=0.9778203918848198\n",
      "SubSGD iter. 125/499: loss=35.567805854926384, w0=38.79999999999999, w1=0.9682066803455055\n",
      "SubSGD iter. 126/499: loss=35.26780585492638, w0=39.09999999999999, w1=0.8530289516733977\n",
      "SubSGD iter. 127/499: loss=34.96780585492639, w0=39.399999999999984, w1=0.8599788017092925\n",
      "SubSGD iter. 128/499: loss=34.6678058549264, w0=39.69999999999998, w1=0.8600413878447336\n",
      "SubSGD iter. 129/499: loss=34.36780585492639, w0=39.99999999999998, w1=0.869090148003342\n",
      "SubSGD iter. 130/499: loss=34.0678058549264, w0=40.299999999999976, w1=0.8055003331790346\n",
      "SubSGD iter. 131/499: loss=33.7678058549264, w0=40.59999999999997, w1=0.8694114279791382\n",
      "SubSGD iter. 132/499: loss=33.467805854926404, w0=40.89999999999997, w1=0.8258848003835559\n",
      "SubSGD iter. 133/499: loss=33.16780585492641, w0=41.19999999999997, w1=0.8341913313708443\n",
      "SubSGD iter. 134/499: loss=32.86780585492641, w0=41.499999999999964, w1=0.8021181456912616\n",
      "SubSGD iter. 135/499: loss=32.56780585492641, w0=41.79999999999996, w1=0.8086277178612948\n",
      "SubSGD iter. 136/499: loss=32.267805854926415, w0=42.09999999999996, w1=0.7775611621480084\n",
      "SubSGD iter. 137/499: loss=31.967805854926418, w0=42.399999999999956, w1=0.6819547266649468\n",
      "SubSGD iter. 138/499: loss=31.667805854926417, w0=42.69999999999995, w1=0.6342265819804636\n",
      "SubSGD iter. 139/499: loss=31.367805854926427, w0=42.99999999999995, w1=0.6724345892429023\n",
      "SubSGD iter. 140/499: loss=31.06780585492643, w0=43.29999999999995, w1=0.6694150145633122\n",
      "SubSGD iter. 141/499: loss=30.767805854926433, w0=43.599999999999945, w1=0.6171428465509926\n",
      "SubSGD iter. 142/499: loss=30.467805854926436, w0=43.89999999999994, w1=0.6703752282838968\n",
      "SubSGD iter. 143/499: loss=30.16780585492643, w0=44.19999999999994, w1=0.5596174074914195\n",
      "SubSGD iter. 144/499: loss=29.867805854926438, w0=44.499999999999936, w1=0.601790194755881\n",
      "SubSGD iter. 145/499: loss=29.56780585492644, w0=44.79999999999993, w1=0.5604298162039465\n",
      "SubSGD iter. 146/499: loss=29.267805854926443, w0=45.09999999999993, w1=0.6323932129737591\n",
      "SubSGD iter. 147/499: loss=28.967805854926446, w0=45.39999999999993, w1=0.5467553267867922\n",
      "SubSGD iter. 148/499: loss=28.66780585492645, w0=45.699999999999925, w1=0.5424369334103522\n",
      "SubSGD iter. 149/499: loss=28.367805854926452, w0=45.99999999999992, w1=0.5417226877436679\n",
      "SubSGD iter. 150/499: loss=28.067805854926455, w0=46.29999999999992, w1=0.44245368391108536\n",
      "SubSGD iter. 151/499: loss=27.767805854926458, w0=46.599999999999916, w1=0.37332077210681325\n",
      "SubSGD iter. 152/499: loss=27.46780585492646, w0=46.89999999999991, w1=0.38120625796556307\n",
      "SubSGD iter. 153/499: loss=27.167805854926463, w0=47.19999999999991, w1=0.4606159923160914\n",
      "SubSGD iter. 154/499: loss=26.868904969556944, w0=47.49999999999991, w1=0.49340651899015586\n",
      "SubSGD iter. 155/499: loss=26.57135306125655, w0=47.799999999999905, w1=0.5621052598808992\n",
      "SubSGD iter. 156/499: loss=26.27322929691234, w0=48.0999999999999, w1=0.5583085699831488\n",
      "SubSGD iter. 157/499: loss=25.976411166880602, w0=48.3812499999999, w1=0.5872850894949966\n",
      "SubSGD iter. 158/499: loss=25.699822354019226, w0=48.6812499999999, w1=0.564088723472868\n",
      "SubSGD iter. 159/499: loss=25.406489918723956, w0=48.981249999999896, w1=0.6331731046230509\n",
      "SubSGD iter. 160/499: loss=25.110265418600154, w0=49.262499999999896, w1=0.6343078240951922\n",
      "SubSGD iter. 161/499: loss=24.836424627433633, w0=49.56249999999989, w1=0.683836289728332\n",
      "SubSGD iter. 162/499: loss=24.543704293979875, w0=49.84374999999989, w1=0.620191659380049\n",
      "SubSGD iter. 163/499: loss=24.279461787890824, w0=50.12499999999989, w1=0.568383118551719\n",
      "SubSGD iter. 164/499: loss=24.018098176924063, w0=50.387499999999896, w1=0.6338904490572269\n",
      "SubSGD iter. 165/499: loss=23.765382445329752, w0=50.68749999999989, w1=0.6251601401026\n",
      "SubSGD iter. 166/499: loss=23.4839785113935, w0=50.98749999999989, w1=0.5645843564851719\n",
      "SubSGD iter. 167/499: loss=23.20791716748062, w0=51.24999999999989, w1=0.6426505638332483\n",
      "SubSGD iter. 168/499: loss=22.95696653979262, w0=51.54999999999989, w1=0.6701571982052825\n",
      "SubSGD iter. 169/499: loss=22.67747224751988, w0=51.83124999999989, w1=0.7070842915199849\n",
      "SubSGD iter. 170/499: loss=22.41590135652778, w0=52.11249999999989, w1=0.741336316774729\n",
      "SubSGD iter. 171/499: loss=22.157000481119553, w0=52.37499999999989, w1=0.8206039721800751\n",
      "SubSGD iter. 172/499: loss=21.909220417720693, w0=52.61874999999989, w1=0.8319320579831636\n",
      "SubSGD iter. 173/499: loss=21.688098953439006, w0=52.86249999999989, w1=0.96826862032052\n",
      "SubSGD iter. 174/499: loss=21.44969240446431, w0=53.12499999999989, w1=1.0156332964450856\n",
      "SubSGD iter. 175/499: loss=21.20861761424404, w0=53.40624999999989, w1=0.9971414335360804\n",
      "SubSGD iter. 176/499: loss=20.965106241819505, w0=53.70624999999989, w1=1.0048064956935028\n",
      "SubSGD iter. 177/499: loss=20.702478357490566, w0=54.00624999999989, w1=1.1237643572838125\n",
      "SubSGD iter. 178/499: loss=20.42197493227779, w0=54.26874999999989, w1=1.2250312596209885\n",
      "SubSGD iter. 179/499: loss=20.178197071196607, w0=54.51249999999989, w1=1.2713475030916865\n",
      "SubSGD iter. 180/499: loss=19.961379913496412, w0=54.75624999999989, w1=1.2948931367128644\n",
      "SubSGD iter. 181/499: loss=19.749119042946703, w0=54.999999999999886, w1=1.4102804300283056\n",
      "SubSGD iter. 182/499: loss=19.518693905434528, w0=55.26249999999989, w1=1.5091730549978593\n",
      "SubSGD iter. 183/499: loss=19.276963282697402, w0=55.48749999999989, w1=1.5760422957828921\n",
      "SubSGD iter. 184/499: loss=19.074509120252376, w0=55.69374999999989, w1=1.6821371592006897\n",
      "SubSGD iter. 185/499: loss=18.880770598100717, w0=55.91874999999989, w1=1.7923644727882935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 186/499: loss=18.67082697565287, w0=56.16249999999989, w1=1.810333331290999\n",
      "SubSGD iter. 187/499: loss=18.46859337373386, w0=56.42499999999989, w1=1.9052221264712066\n",
      "SubSGD iter. 188/499: loss=18.235872679863242, w0=56.68749999999989, w1=1.8456256437817813\n",
      "SubSGD iter. 189/499: loss=18.050623045042048, w0=56.949999999999896, w1=1.9914287016277705\n",
      "SubSGD iter. 190/499: loss=17.80985264163026, w0=57.231249999999896, w1=2.0232887742093384\n",
      "SubSGD iter. 191/499: loss=17.586552447060747, w0=57.474999999999895, w1=2.0970974164345257\n",
      "SubSGD iter. 192/499: loss=17.381109304304637, w0=57.68124999999989, w1=2.1707128751756777\n",
      "SubSGD iter. 193/499: loss=17.210013144308274, w0=57.96249999999989, w1=2.239193278205186\n",
      "SubSGD iter. 194/499: loss=16.987904636248032, w0=58.131249999999895, w1=2.441217150526274\n",
      "SubSGD iter. 195/499: loss=16.8038457944291, w0=58.33749999999989, w1=2.5832182070922767\n",
      "SubSGD iter. 196/499: loss=16.611998394495554, w0=58.48749999999989, w1=2.707399962146054\n",
      "SubSGD iter. 197/499: loss=16.466240751646737, w0=58.67499999999989, w1=2.7912160030762903\n",
      "SubSGD iter. 198/499: loss=16.308118502911718, w0=58.91874999999989, w1=2.81802091094742\n",
      "SubSGD iter. 199/499: loss=16.130532223354923, w0=59.124999999999886, w1=2.9122109581594087\n",
      "SubSGD iter. 200/499: loss=15.958093766884943, w0=59.33124999999988, w1=2.9828413685394337\n",
      "SubSGD iter. 201/499: loss=15.793543117676549, w0=59.556249999999885, w1=3.0360674690068588\n",
      "SubSGD iter. 202/499: loss=15.62423710066892, w0=59.79999999999988, w1=3.1721545885191333\n",
      "SubSGD iter. 203/499: loss=15.418294038991034, w0=59.968749999999886, w1=3.336259936661672\n",
      "SubSGD iter. 204/499: loss=15.251308682719836, w0=60.17499999999988, w1=3.395152834539812\n",
      "SubSGD iter. 205/499: loss=15.10366304762328, w0=60.306249999999885, w1=3.472920059868603\n",
      "SubSGD iter. 206/499: loss=14.995079359045787, w0=60.399999999999885, w1=3.554425860143905\n",
      "SubSGD iter. 207/499: loss=14.907301192621214, w0=60.54999999999988, w1=3.6210934351045\n",
      "SubSGD iter. 208/499: loss=14.792277379699534, w0=60.75624999999988, w1=3.671778737158036\n",
      "SubSGD iter. 209/499: loss=14.651566013795126, w0=60.94374999999988, w1=3.797871573819199\n",
      "SubSGD iter. 210/499: loss=14.491712190552978, w0=61.11249999999988, w1=3.968404117022605\n",
      "SubSGD iter. 211/499: loss=14.325075285377652, w0=61.29999999999988, w1=4.138960949817963\n",
      "SubSGD iter. 212/499: loss=14.147475715838775, w0=61.50624999999988, w1=4.269170476408027\n",
      "SubSGD iter. 213/499: loss=13.975189034871626, w0=61.58124999999988, w1=4.442668226771851\n",
      "SubSGD iter. 214/499: loss=13.861970457131083, w0=61.82499999999988, w1=4.516413302545777\n",
      "SubSGD iter. 215/499: loss=13.692977902002838, w0=61.993749999999885, w1=4.645635826207973\n",
      "SubSGD iter. 216/499: loss=13.544575512960098, w0=62.19999999999988, w1=4.675109881970431\n",
      "SubSGD iter. 217/499: loss=13.419777384073988, w0=62.34999999999988, w1=4.798855818621167\n",
      "SubSGD iter. 218/499: loss=13.287642907620437, w0=62.48124999999988, w1=4.932775406573865\n",
      "SubSGD iter. 219/499: loss=13.161263278592045, w0=62.61249999999988, w1=5.057652437266121\n",
      "SubSGD iter. 220/499: loss=13.038781358496788, w0=62.76249999999988, w1=5.246775320696872\n",
      "SubSGD iter. 221/499: loss=12.879764722803221, w0=62.931249999999885, w1=5.327231775494213\n",
      "SubSGD iter. 222/499: loss=12.757050001038344, w0=63.08124999999988, w1=5.486309543995985\n",
      "SubSGD iter. 223/499: loss=12.610790868757654, w0=63.23124999999988, w1=5.6160402618298235\n",
      "SubSGD iter. 224/499: loss=12.476992833728183, w0=63.34374999999988, w1=5.780837074384035\n",
      "SubSGD iter. 225/499: loss=12.347983546996785, w0=63.47499999999988, w1=5.889084011930327\n",
      "SubSGD iter. 226/499: loss=12.233146882549951, w0=63.64374999999988, w1=5.973916676146884\n",
      "SubSGD iter. 227/499: loss=12.108836958195278, w0=63.812499999999886, w1=6.211062135624601\n",
      "SubSGD iter. 228/499: loss=11.919520910147645, w0=63.98124999999989, w1=6.3275015175987965\n",
      "SubSGD iter. 229/499: loss=11.784500330690028, w0=64.14999999999989, w1=6.513219083311023\n",
      "SubSGD iter. 230/499: loss=11.619088269534055, w0=64.28124999999989, w1=6.649560173322511\n",
      "SubSGD iter. 231/499: loss=11.494007883256517, w0=64.44999999999989, w1=6.837730745819066\n",
      "SubSGD iter. 232/499: loss=11.327514429006794, w0=64.61874999999989, w1=6.991906858212844\n",
      "SubSGD iter. 233/499: loss=11.176007225352036, w0=64.6937499999999, w1=7.135124580513833\n",
      "SubSGD iter. 234/499: loss=11.075741852285102, w0=64.91874999999989, w1=7.160280339233804\n",
      "SubSGD iter. 235/499: loss=10.954555071377106, w0=65.08749999999989, w1=7.309638827993778\n",
      "SubSGD iter. 236/499: loss=10.805695321153863, w0=65.27499999999989, w1=7.5148375755049255\n",
      "SubSGD iter. 237/499: loss=10.624531451330125, w0=65.4249999999999, w1=7.610229211040315\n",
      "SubSGD iter. 238/499: loss=10.51170685167432, w0=65.5374999999999, w1=7.7489980727474315\n",
      "SubSGD iter. 239/499: loss=10.396769167051321, w0=65.64999999999989, w1=7.838813260969745\n",
      "SubSGD iter. 240/499: loss=10.305045123346645, w0=65.74374999999989, w1=8.023202173577173\n",
      "SubSGD iter. 241/499: loss=10.17860837749646, w0=65.83749999999989, w1=8.198836513937573\n",
      "SubSGD iter. 242/499: loss=10.056191520188179, w0=65.94999999999989, w1=8.348496200112038\n",
      "SubSGD iter. 243/499: loss=9.937347637552268, w0=66.0999999999999, w1=8.491932126216394\n",
      "SubSGD iter. 244/499: loss=9.8046536347212, w0=66.21249999999989, w1=8.62222106686586\n",
      "SubSGD iter. 245/499: loss=9.694704331487555, w0=66.3812499999999, w1=8.736244819427355\n",
      "SubSGD iter. 246/499: loss=9.56752828917927, w0=66.5312499999999, w1=8.934604811573253\n",
      "SubSGD iter. 247/499: loss=9.409866012598274, w0=66.6812499999999, w1=9.084107065476589\n",
      "SubSGD iter. 248/499: loss=9.2749418814768, w0=66.8124999999999, w1=9.232772340621034\n",
      "SubSGD iter. 249/499: loss=9.1497555749638, w0=66.9437499999999, w1=9.380868058626078\n",
      "SubSGD iter. 250/499: loss=9.027293259915535, w0=67.05624999999989, w1=9.509629861742432\n",
      "SubSGD iter. 251/499: loss=8.921643918447367, w0=67.16874999999989, w1=9.685102048473192\n",
      "SubSGD iter. 252/499: loss=8.795128322496808, w0=67.28124999999989, w1=9.804365936708772\n",
      "SubSGD iter. 253/499: loss=8.695245634718383, w0=67.46874999999989, w1=9.885781285117348\n",
      "SubSGD iter. 254/499: loss=8.5813293975956, w0=67.54374999999989, w1=9.961387708338298\n",
      "SubSGD iter. 255/499: loss=8.517697851414182, w0=67.63749999999989, w1=10.145982380760165\n",
      "SubSGD iter. 256/499: loss=8.400459032500116, w0=67.82499999999989, w1=10.284213254973677\n",
      "SubSGD iter. 257/499: loss=8.266743151073435, w0=67.89999999999989, w1=10.36912671488043\n",
      "SubSGD iter. 258/499: loss=8.20167944141751, w0=68.08749999999989, w1=10.494535804110129\n",
      "SubSGD iter. 259/499: loss=8.078119641187639, w0=68.19999999999989, w1=10.58375667691498\n",
      "SubSGD iter. 260/499: loss=7.998420534150361, w0=68.29374999999989, w1=10.7068665877453\n",
      "SubSGD iter. 261/499: loss=7.913332024935621, w0=68.36874999999989, w1=10.76149540443501\n",
      "SubSGD iter. 262/499: loss=7.863051382306324, w0=68.5187499999999, w1=10.884826976529823\n",
      "SubSGD iter. 263/499: loss=7.756806086166534, w0=68.6124999999999, w1=11.063437866055487\n",
      "SubSGD iter. 264/499: loss=7.649398619203111, w0=68.7062499999999, w1=11.2832585526247\n",
      "SubSGD iter. 265/499: loss=7.527029227918171, w0=68.83749999999989, w1=11.425762037320418\n",
      "SubSGD iter. 266/499: loss=7.422550299095425, w0=68.85624999999989, w1=11.53982979474095\n",
      "SubSGD iter. 267/499: loss=7.371651018407273, w0=68.94999999999989, w1=11.641435197156174\n",
      "SubSGD iter. 268/499: loss=7.297315136952486, w0=69.04374999999989, w1=11.790179282796679\n",
      "SubSGD iter. 269/499: loss=7.207093972052415, w0=69.21249999999989, w1=11.828592064977757\n",
      "SubSGD iter. 270/499: loss=7.130397686280685, w0=69.23124999999989, w1=11.963593457660519\n",
      "SubSGD iter. 271/499: loss=7.074334961534654, w0=69.36249999999988, w1=12.015357180383486\n",
      "SubSGD iter. 272/499: loss=7.009202810613105, w0=69.45624999999988, w1=12.093489448266938\n",
      "SubSGD iter. 273/499: loss=6.949610924930762, w0=69.54999999999988, w1=12.182266306732108\n",
      "SubSGD iter. 274/499: loss=6.886199947880353, w0=69.68124999999988, w1=12.362748723320719\n",
      "SubSGD iter. 275/499: loss=6.778919194947124, w0=69.83124999999988, w1=12.525181109203517\n",
      "SubSGD iter. 276/499: loss=6.67160818635806, w0=69.90624999999989, w1=12.677134356490258\n",
      "SubSGD iter. 277/499: loss=6.594089608304107, w0=70.07499999999989, w1=12.788148211061522\n",
      "SubSGD iter. 278/499: loss=6.501337926370126, w0=70.16874999999989, w1=12.942175417355708\n",
      "SubSGD iter. 279/499: loss=6.418516702393077, w0=70.2249999999999, w1=12.96318326317862\n",
      "SubSGD iter. 280/499: loss=6.393019765065337, w0=70.2999999999999, w1=13.059247668543595\n",
      "SubSGD iter. 281/499: loss=6.335964689944105, w0=70.3562499999999, w1=13.141244186839781\n",
      "SubSGD iter. 282/499: loss=6.289865583899824, w0=70.4499999999999, w1=13.2241526717484\n",
      "SubSGD iter. 283/499: loss=6.232980540166319, w0=70.5249999999999, w1=13.398774505363871\n",
      "SubSGD iter. 284/499: loss=6.151799181692993, w0=70.50624999999991, w1=13.558026947236883\n",
      "SubSGD iter. 285/499: loss=6.106919153562875, w0=70.58124999999991, w1=13.649202704295853\n",
      "SubSGD iter. 286/499: loss=6.05614511377859, w0=70.65624999999991, w1=13.692911426168306\n",
      "SubSGD iter. 287/499: loss=6.020312278328724, w0=70.73124999999992, w1=13.73966058445337\n",
      "SubSGD iter. 288/499: loss=5.984390203550661, w0=70.78749999999992, w1=13.824945628629994\n",
      "SubSGD iter. 289/499: loss=5.94229168227077, w0=70.82499999999992, w1=13.916077278455182\n",
      "SubSGD iter. 290/499: loss=5.9035742017668165, w0=70.91874999999992, w1=13.990350011055657\n",
      "SubSGD iter. 291/499: loss=5.855864346301224, w0=71.01249999999992, w1=14.14430582562159\n",
      "SubSGD iter. 292/499: loss=5.784483191605507, w0=71.10624999999992, w1=14.181364696325925\n",
      "SubSGD iter. 293/499: loss=5.749389470136842, w0=71.19999999999992, w1=14.301413504133814\n",
      "SubSGD iter. 294/499: loss=5.692392970164127, w0=71.25624999999992, w1=14.405052577644287\n",
      "SubSGD iter. 295/499: loss=5.650588710023216, w0=71.36874999999992, w1=14.482487447386172\n",
      "SubSGD iter. 296/499: loss=5.6044717109847255, w0=71.40624999999991, w1=14.494705546610113\n",
      "SubSGD iter. 297/499: loss=5.593885273204789, w0=71.44374999999991, w1=14.552246336891088\n",
      "SubSGD iter. 298/499: loss=5.572688406487077, w0=71.51874999999991, w1=14.640495170975655\n",
      "SubSGD iter. 299/499: loss=5.537854139003891, w0=71.61249999999991, w1=14.753655920054277\n",
      "SubSGD iter. 300/499: loss=5.499531130471027, w0=71.70624999999991, w1=14.75656194279061\n",
      "SubSGD iter. 301/499: loss=5.486667512905952, w0=71.70624999999991, w1=14.758388496965116\n",
      "SubSGD iter. 302/499: loss=5.486378407463118, w0=71.72499999999991, w1=14.864489985505644\n",
      "SubSGD iter. 303/499: loss=5.46728392252423, w0=71.72499999999991, w1=14.934638341848949\n",
      "SubSGD iter. 304/499: loss=5.457090305017477, w0=71.7437499999999, w1=15.02845373032222\n",
      "SubSGD iter. 305/499: loss=5.441170925353305, w0=71.7437499999999, w1=15.084323932732707\n",
      "SubSGD iter. 306/499: loss=5.434587480834522, w0=71.7624999999999, w1=15.12417095824337\n",
      "SubSGD iter. 307/499: loss=5.427834277271951, w0=71.7999999999999, w1=15.169557360790916\n",
      "SubSGD iter. 308/499: loss=5.417922033131749, w0=71.7624999999999, w1=15.088837435450534\n",
      "SubSGD iter. 309/499: loss=5.431517922275467, w0=71.81874999999991, w1=15.108032378527449\n",
      "SubSGD iter. 310/499: loss=5.421719751713357, w0=71.96874999999991, w1=15.209721967407077\n",
      "SubSGD iter. 311/499: loss=5.392178328521785, w0=72.04374999999992, w1=15.196479660106158\n",
      "SubSGD iter. 312/499: loss=5.385286360218097, w0=72.11874999999992, w1=15.255773268395528\n",
      "SubSGD iter. 313/499: loss=5.372022538946168, w0=72.11874999999992, w1=15.347494350658723\n",
      "SubSGD iter. 314/499: loss=5.363382279612988, w0=72.06249999999991, w1=15.384063367686426\n",
      "SubSGD iter. 315/499: loss=5.366610944038086, w0=72.09999999999991, w1=15.378629219659993\n",
      "SubSGD iter. 316/499: loss=5.362957364373296, w0=72.1374999999999, w1=15.40798664415353\n",
      "SubSGD iter. 317/499: loss=5.357052728921925, w0=72.1562499999999, w1=15.409181199725852\n",
      "SubSGD iter. 318/499: loss=5.355112102732394, w0=72.1562499999999, w1=15.381994021818072\n",
      "SubSGD iter. 319/499: loss=5.357028215640293, w0=72.1562499999999, w1=15.436969778313395\n",
      "SubSGD iter. 320/499: loss=5.3531536039759455, w0=72.2687499999999, w1=15.493054241146995\n",
      "SubSGD iter. 321/499: loss=5.339003836334411, w0=72.2312499999999, w1=15.465334311318522\n",
      "SubSGD iter. 322/499: loss=5.343969268644235, w0=72.1749999999999, w1=15.479261190053768\n",
      "SubSGD iter. 323/499: loss=5.348316530908231, w0=72.11874999999989, w1=15.494641575877907\n",
      "SubSGD iter. 324/499: loss=5.35280185053042, w0=72.11874999999989, w1=15.51002176068041\n",
      "SubSGD iter. 325/499: loss=5.35171787738963, w0=72.13749999999989, w1=15.56427899690117\n",
      "SubSGD iter. 326/499: loss=5.346526997023485, w0=72.13749999999989, w1=15.552793782419405\n",
      "SubSGD iter. 327/499: loss=5.347199806962766, w0=72.15624999999989, w1=15.579861296643985\n",
      "SubSGD iter. 328/499: loss=5.344007484947883, w0=72.21249999999989, w1=15.535149507528137\n",
      "SubSGD iter. 329/499: loss=5.341550249601747, w0=72.1937499999999, w1=15.56517988801835\n",
      "SubSGD iter. 330/499: loss=5.341670340564647, w0=72.21249999999989, w1=15.66955784350577\n",
      "SubSGD iter. 331/499: loss=5.336728526523808, w0=72.21249999999989, w1=15.679808609051406\n",
      "SubSGD iter. 332/499: loss=5.336486885335296, w0=72.2687499999999, w1=15.715623708316558\n",
      "SubSGD iter. 333/499: loss=5.331271904471119, w0=72.2874999999999, w1=15.778988305972446\n",
      "SubSGD iter. 334/499: loss=5.328208329322351, w0=72.32499999999989, w1=15.743139630681716\n",
      "SubSGD iter. 335/499: loss=5.326483048345441, w0=72.36249999999988, w1=15.789111753372389\n",
      "SubSGD iter. 336/499: loss=5.322658840042465, w0=72.32499999999989, w1=15.801632621220602\n",
      "SubSGD iter. 337/499: loss=5.324704238087204, w0=72.39999999999989, w1=15.708315992612908\n",
      "SubSGD iter. 338/499: loss=5.32253272459266, w0=72.39999999999989, w1=15.773938384629428\n",
      "SubSGD iter. 339/499: loss=5.320825797171174, w0=72.41874999999989, w1=15.75632464229464\n",
      "SubSGD iter. 340/499: loss=5.3202852324205825, w0=72.45624999999988, w1=15.737080081031898\n",
      "SubSGD iter. 341/499: loss=5.31893228702434, w0=72.47499999999988, w1=15.723527049254146\n",
      "SubSGD iter. 342/499: loss=5.31839709627201, w0=72.43749999999989, w1=15.743819409826221\n",
      "SubSGD iter. 343/499: loss=5.319641766206283, w0=72.36249999999988, w1=15.733765879889745\n",
      "SubSGD iter. 344/499: loss=5.324187347819778, w0=72.41874999999989, w1=15.697674394423203\n",
      "SubSGD iter. 345/499: loss=5.321695665608876, w0=72.39999999999989, w1=15.66296278045775\n",
      "SubSGD iter. 346/499: loss=5.323879335870031, w0=72.39999999999989, w1=15.658025606093378\n",
      "SubSGD iter. 347/499: loss=5.32403908955941, w0=72.39999999999989, w1=15.673258736242476\n",
      "SubSGD iter. 348/499: loss=5.323546186420387, w0=72.4749999999999, w1=15.74297668650457\n",
      "SubSGD iter. 349/499: loss=5.318099485079772, w0=72.4562499999999, w1=15.785212461835622\n",
      "SubSGD iter. 350/499: loss=5.318195783044762, w0=72.4749999999999, w1=15.764671064571232\n",
      "SubSGD iter. 351/499: loss=5.317767525689569, w0=72.5687499999999, w1=15.81971587257712\n",
      "SubSGD iter. 352/499: loss=5.313448685087694, w0=72.6249999999999, w1=15.813929709832875\n",
      "SubSGD iter. 353/499: loss=5.311883194533455, w0=72.6624999999999, w1=15.742394233828994\n",
      "SubSGD iter. 354/499: loss=5.312280713018564, w0=72.6624999999999, w1=15.730458404425338\n",
      "SubSGD iter. 355/499: loss=5.312487639308596, w0=72.68124999999989, w1=15.739902508120515\n",
      "SubSGD iter. 356/499: loss=5.312217028897775, w0=72.6437499999999, w1=15.632068165062305\n",
      "SubSGD iter. 357/499: loss=5.317060782018641, w0=72.68124999999989, w1=15.634555882549275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 358/499: loss=5.317302010247017, w0=72.5499999999999, w1=15.623424276668485\n",
      "SubSGD iter. 359/499: loss=5.317895164169402, w0=72.58749999999989, w1=15.597405870386302\n",
      "SubSGD iter. 360/499: loss=5.318980705625352, w0=72.60624999999989, w1=15.687381905463466\n",
      "SubSGD iter. 361/499: loss=5.314752706096126, w0=72.64374999999988, w1=15.748595596276298\n",
      "SubSGD iter. 362/499: loss=5.3125152697661076, w0=72.69999999999989, w1=15.789318308091893\n",
      "SubSGD iter. 363/499: loss=5.311576591629316, w0=72.71874999999989, w1=15.742295274957057\n",
      "SubSGD iter. 364/499: loss=5.31264803735489, w0=72.68124999999989, w1=15.750674875983819\n",
      "SubSGD iter. 365/499: loss=5.311963399218037, w0=72.69999999999989, w1=15.77837179439256\n",
      "SubSGD iter. 366/499: loss=5.311645945133056, w0=72.77499999999989, w1=15.764299242607152\n",
      "SubSGD iter. 367/499: loss=5.313895519737478, w0=72.69999999999989, w1=15.740959533596024\n",
      "SubSGD iter. 368/499: loss=5.312252590278848, w0=72.6624999999999, w1=15.738010591907916\n",
      "SubSGD iter. 369/499: loss=5.312350417173898, w0=72.6437499999999, w1=15.707616407782117\n",
      "SubSGD iter. 370/499: loss=5.313383837882222, w0=72.6437499999999, w1=15.75283901072905\n",
      "SubSGD iter. 371/499: loss=5.312438044490444, w0=72.7187499999999, w1=15.76287045316947\n",
      "SubSGD iter. 372/499: loss=5.312103983947061, w0=72.7374999999999, w1=15.817237938269384\n",
      "SubSGD iter. 373/499: loss=5.311737224260645, w0=72.6999999999999, w1=15.767828930873792\n",
      "SubSGD iter. 374/499: loss=5.311721900343569, w0=72.6812499999999, w1=15.787758142223952\n",
      "SubSGD iter. 375/499: loss=5.31158540046119, w0=72.6999999999999, w1=15.716262114172647\n",
      "SubSGD iter. 376/499: loss=5.313393707957962, w0=72.6437499999999, w1=15.781172672652792\n",
      "SubSGD iter. 377/499: loss=5.311922404343349, w0=72.6999999999999, w1=15.827741687717634\n",
      "SubSGD iter. 378/499: loss=5.311359649893746, w0=72.7187499999999, w1=15.928232830225287\n",
      "SubSGD iter. 379/499: loss=5.311323123190512, w0=72.7187499999999, w1=15.935321067231742\n",
      "SubSGD iter. 380/499: loss=5.311344652189341, w0=72.6999999999999, w1=15.885204126119877\n",
      "SubSGD iter. 381/499: loss=5.3110352119802045, w0=72.75624999999991, w1=15.940491450580241\n",
      "SubSGD iter. 382/499: loss=5.312043486128466, w0=72.7937499999999, w1=15.926448288949201\n",
      "SubSGD iter. 383/499: loss=5.312765291212971, w0=72.7374999999999, w1=15.90072034523497\n",
      "SubSGD iter. 384/499: loss=5.311613379412975, w0=72.7374999999999, w1=15.915922557859954\n",
      "SubSGD iter. 385/499: loss=5.311635862767586, w0=72.8124999999999, w1=15.851860753174206\n",
      "SubSGD iter. 386/499: loss=5.313026266898365, w0=72.7937499999999, w1=15.85221910655549\n",
      "SubSGD iter. 387/499: loss=5.312655509757383, w0=72.84999999999991, w1=15.856235643513493\n",
      "SubSGD iter. 388/499: loss=5.314077111244511, w0=72.84999999999991, w1=15.818157804634769\n",
      "SubSGD iter. 389/499: loss=5.315349305408072, w0=72.7749999999999, w1=15.890112260771039\n",
      "SubSGD iter. 390/499: loss=5.312340264814405, w0=72.75624999999991, w1=15.89423274878168\n",
      "SubSGD iter. 391/499: loss=5.311975071692886, w0=72.71874999999991, w1=15.954269531182034\n",
      "SubSGD iter. 392/499: loss=5.311402204082406, w0=72.71874999999991, w1=15.949963498155052\n",
      "SubSGD iter. 393/499: loss=5.311389125431126, w0=72.71874999999991, w1=15.943623691762575\n",
      "SubSGD iter. 394/499: loss=5.311369869629679, w0=72.69999999999992, w1=16.014421854762787\n",
      "SubSGD iter. 395/499: loss=5.31139926029273, w0=72.68124999999992, w1=16.08376897998498\n",
      "SubSGD iter. 396/499: loss=5.3114242437399115, w0=72.64374999999993, w1=16.085299152979694\n",
      "SubSGD iter. 397/499: loss=5.311057604183307, w0=72.66249999999992, w1=16.108998228959592\n",
      "SubSGD iter. 398/499: loss=5.3113152286053245, w0=72.68124999999992, w1=16.188743032928187\n",
      "SubSGD iter. 399/499: loss=5.311819822115508, w0=72.68124999999992, w1=16.236513694578708\n",
      "SubSGD iter. 400/499: loss=5.313045695377964, w0=72.62499999999991, w1=16.14919035750113\n",
      "SubSGD iter. 401/499: loss=5.31142396778415, w0=72.69999999999992, w1=16.02565341356035\n",
      "SubSGD iter. 402/499: loss=5.311433373742378, w0=72.66249999999992, w1=16.086241910466864\n",
      "SubSGD iter. 403/499: loss=5.311246111171224, w0=72.69999999999992, w1=16.14634912361455\n",
      "SubSGD iter. 404/499: loss=5.311799961061356, w0=72.73749999999991, w1=16.071143790042324\n",
      "SubSGD iter. 405/499: loss=5.311942828127792, w0=72.6812499999999, w1=16.010997847786115\n",
      "SubSGD iter. 406/499: loss=5.311203217041946, w0=72.66249999999991, w1=15.996360244219641\n",
      "SubSGD iter. 407/499: loss=5.310973114897866, w0=72.6812499999999, w1=16.036258184480687\n",
      "SubSGD iter. 408/499: loss=5.3112799398939945, w0=72.64374999999991, w1=16.107045779974985\n",
      "SubSGD iter. 409/499: loss=5.311123654896157, w0=72.66249999999991, w1=16.218086102009757\n",
      "SubSGD iter. 410/499: loss=5.312188046441334, w0=72.5874999999999, w1=16.27918765753552\n",
      "SubSGD iter. 411/499: loss=5.3138999765611, w0=72.66249999999991, w1=16.186144000155853\n",
      "SubSGD iter. 412/499: loss=5.311699495645341, w0=72.62499999999991, w1=16.23783225930487\n",
      "SubSGD iter. 413/499: loss=5.312728672730772, w0=72.66249999999991, w1=16.234883878825855\n",
      "SubSGD iter. 414/499: loss=5.31266097260972, w0=72.56874999999991, w1=16.239265246948644\n",
      "SubSGD iter. 415/499: loss=5.313811862892608, w0=72.56874999999991, w1=16.20700814654106\n",
      "SubSGD iter. 416/499: loss=5.313411199231111, w0=72.6062499999999, w1=16.266672594912528\n",
      "SubSGD iter. 417/499: loss=5.313353715611437, w0=72.6999999999999, w1=16.20850510912937\n",
      "SubSGD iter. 418/499: loss=5.312552641927213, w0=72.75624999999991, w1=16.171678773874408\n",
      "SubSGD iter. 419/499: loss=5.313377725621898, w0=72.7749999999999, w1=16.153640668783442\n",
      "SubSGD iter. 420/499: loss=5.313685744411523, w0=72.84999999999991, w1=16.104429623531992\n",
      "SubSGD iter. 421/499: loss=5.315602608195635, w0=72.7749999999999, w1=16.09485947595344\n",
      "SubSGD iter. 422/499: loss=5.312944638697642, w0=72.73749999999991, w1=16.150396092037024\n",
      "SubSGD iter. 423/499: loss=5.312529908626874, w0=72.73749999999991, w1=16.157319813696134\n",
      "SubSGD iter. 424/499: loss=5.312602812091424, w0=72.7749999999999, w1=16.140933509749342\n",
      "SubSGD iter. 425/499: loss=5.313413729384914, w0=72.66249999999991, w1=16.062328219208826\n",
      "SubSGD iter. 426/499: loss=5.311173478465709, w0=72.62499999999991, w1=16.072986437760072\n",
      "SubSGD iter. 427/499: loss=5.310834563388679, w0=72.60624999999992, w1=15.998022982075497\n",
      "SubSGD iter. 428/499: loss=5.310738174399238, w0=72.66249999999992, w1=15.981785655513024\n",
      "SubSGD iter. 429/499: loss=5.310928847712257, w0=72.73749999999993, w1=16.04205538927897\n",
      "SubSGD iter. 430/499: loss=5.311854478352606, w0=72.64374999999993, w1=16.093129972853898\n",
      "SubSGD iter. 431/499: loss=5.311081388618246, w0=72.62499999999993, w1=16.129317274035934\n",
      "SubSGD iter. 432/499: loss=5.311131459277744, w0=72.64374999999993, w1=16.173991427286815\n",
      "SubSGD iter. 433/499: loss=5.311603366911484, w0=72.66249999999992, w1=16.080341249262336\n",
      "SubSGD iter. 434/499: loss=5.311228189179048, w0=72.62499999999993, w1=16.061494846528483\n",
      "SubSGD iter. 435/499: loss=5.310799660146315, w0=72.71874999999993, w1=16.054381328554875\n",
      "SubSGD iter. 436/499: loss=5.311706272183755, w0=72.66249999999992, w1=16.063369457029648\n",
      "SubSGD iter. 437/499: loss=5.311176641002149, w0=72.69999999999992, w1=16.04421075383203\n",
      "SubSGD iter. 438/499: loss=5.3114897376812, w0=72.62499999999991, w1=16.091334970624818\n",
      "SubSGD iter. 439/499: loss=5.310890293119811, w0=72.73749999999991, w1=16.130974415091845\n",
      "SubSGD iter. 440/499: loss=5.3123254076942255, w0=72.73749999999991, w1=16.18192935373199\n",
      "SubSGD iter. 441/499: loss=5.313056197155842, w0=72.73749999999991, w1=16.168104295067927\n",
      "SubSGD iter. 442/499: loss=5.312738707838815, w0=72.71874999999991, w1=16.13962446931897\n",
      "SubSGD iter. 443/499: loss=5.312045201483082, w0=72.66249999999991, w1=16.141790234706654\n",
      "SubSGD iter. 444/499: loss=5.311414827285886, w0=72.5874999999999, w1=16.14014294928482\n",
      "SubSGD iter. 445/499: loss=5.312023739618181, w0=72.56874999999991, w1=16.114271578587374\n",
      "SubSGD iter. 446/499: loss=5.312259323445579, w0=72.56874999999991, w1=16.04394153551953\n",
      "SubSGD iter. 447/499: loss=5.311556738985727, w0=72.54999999999991, w1=16.006186988170402\n",
      "SubSGD iter. 448/499: loss=5.312253759178531, w0=72.54999999999991, w1=15.977113745355739\n",
      "SubSGD iter. 449/499: loss=5.312400484554014, w0=72.53124999999991, w1=15.920615310572678\n",
      "SubSGD iter. 450/499: loss=5.313242548698158, w0=72.53124999999991, w1=15.971162475239185\n",
      "SubSGD iter. 451/499: loss=5.312987449816946, w0=72.49374999999992, w1=15.937191089379464\n",
      "SubSGD iter. 452/499: loss=5.314385113495469, w0=72.53124999999991, w1=16.00713960945814\n",
      "SubSGD iter. 453/499: loss=5.312805882230532, w0=72.54999999999991, w1=15.97821054475735\n",
      "SubSGD iter. 454/499: loss=5.312394949282112, w0=72.51249999999992, w1=16.059435373090608\n",
      "SubSGD iter. 455/499: loss=5.313620941736068, w0=72.56874999999992, w1=16.097596777080085\n",
      "SubSGD iter. 456/499: loss=5.312052206657578, w0=72.62499999999993, w1=16.07431678582677\n",
      "SubSGD iter. 457/499: loss=5.3108386040354505, w0=72.66249999999992, w1=16.013977656357365\n",
      "SubSGD iter. 458/499: loss=5.311026624006682, w0=72.77499999999992, w1=16.02651404418398\n",
      "SubSGD iter. 459/499: loss=5.312541996611049, w0=72.81249999999991, w1=16.04231176502374\n",
      "SubSGD iter. 460/499: loss=5.3135870847317275, w0=72.75624999999991, w1=16.122736063079376\n",
      "SubSGD iter. 461/499: loss=5.312637794086375, w0=72.7749999999999, w1=16.131459740463622\n",
      "SubSGD iter. 462/499: loss=5.313272986522393, w0=72.8124999999999, w1=16.09843142326352\n",
      "SubSGD iter. 463/499: loss=5.314090544691795, w0=72.7749999999999, w1=16.085347378780103\n",
      "SubSGD iter. 464/499: loss=5.312859303893169, w0=72.75624999999991, w1=16.02218607315548\n",
      "SubSGD iter. 465/499: loss=5.312164308617556, w0=72.75624999999991, w1=16.06045417651985\n",
      "SubSGD iter. 466/499: loss=5.312220905335376, w0=72.7937499999999, w1=15.982912636063785\n",
      "SubSGD iter. 467/499: loss=5.312848799316053, w0=72.7374999999999, w1=16.026248360713257\n",
      "SubSGD iter. 468/499: loss=5.311806467895397, w0=72.8312499999999, w1=16.004287198642018\n",
      "SubSGD iter. 469/499: loss=5.313802889921319, w0=72.77499999999989, w1=16.01421566430914\n",
      "SubSGD iter. 470/499: loss=5.312523807887955, w0=72.69999999999989, w1=16.019447947761265\n",
      "SubSGD iter. 471/499: loss=5.311414525971655, w0=72.77499999999989, w1=16.061952507442097\n",
      "SubSGD iter. 472/499: loss=5.312649424122709, w0=72.69999999999989, w1=16.06506756365024\n",
      "SubSGD iter. 473/499: loss=5.311553085765321, w0=72.68124999999989, w1=16.030555852799683\n",
      "SubSGD iter. 474/499: loss=5.311262620285187, w0=72.58749999999989, w1=16.032904347955913\n",
      "SubSGD iter. 475/499: loss=5.311145313771058, w0=72.5499999999999, w1=16.015749336127957\n",
      "SubSGD iter. 476/499: loss=5.312205500403152, w0=72.4749999999999, w1=15.984496671778901\n",
      "SubSGD iter. 477/499: loss=5.314826001417804, w0=72.51249999999989, w1=15.94340434225424\n",
      "SubSGD iter. 478/499: loss=5.313684468855539, w0=72.53124999999989, w1=15.975453496582869\n",
      "SubSGD iter. 479/499: loss=5.312965794106853, w0=72.54999999999988, w1=16.011452940275575\n",
      "SubSGD iter. 480/499: loss=5.312227183237045, w0=72.51249999999989, w1=15.88451196065852\n",
      "SubSGD iter. 481/499: loss=5.314448615881164, w0=72.51249999999989, w1=15.817431646631771\n",
      "SubSGD iter. 482/499: loss=5.315475054162569, w0=72.5687499999999, w1=15.885630704261645\n",
      "SubSGD iter. 483/499: loss=5.312350582110861, w0=72.5687499999999, w1=15.911451437332762\n",
      "SubSGD iter. 484/499: loss=5.3121749350852925, w0=72.5312499999999, w1=15.879680153755054\n",
      "SubSGD iter. 485/499: loss=5.313779976154995, w0=72.5312499999999, w1=15.86436855553264\n",
      "SubSGD iter. 486/499: loss=5.314014268596718, w0=72.5874999999999, w1=15.92863157273141\n",
      "SubSGD iter. 487/499: loss=5.311637697351758, w0=72.54999999999991, w1=15.97865437005503\n",
      "SubSGD iter. 488/499: loss=5.312392709406986, w0=72.62499999999991, w1=15.979132796836545\n",
      "SubSGD iter. 489/499: loss=5.310590901157521, w0=72.68124999999992, w1=15.992834006770291\n",
      "SubSGD iter. 490/499: loss=5.311148048272672, w0=72.64374999999993, w1=16.024484759345036\n",
      "SubSGD iter. 491/499: loss=5.3108728935129115, w0=72.69999999999993, w1=16.03137480222465\n",
      "SubSGD iter. 492/499: loss=5.311450751232681, w0=72.69999999999993, w1=15.994175761531494\n",
      "SubSGD iter. 493/499: loss=5.311337767129186, w0=72.68124999999993, w1=15.996621565176602\n",
      "SubSGD iter. 494/499: loss=5.311159552168553, w0=72.68124999999993, w1=16.040274322872403\n",
      "SubSGD iter. 495/499: loss=5.311292138052529, w0=72.73749999999994, w1=16.14805027572482\n",
      "SubSGD iter. 496/499: loss=5.312505208307483, w0=72.77499999999993, w1=16.15220843020793\n",
      "SubSGD iter. 497/499: loss=5.313655085283612, w0=72.79374999999993, w1=16.189164579687315\n",
      "SubSGD iter. 498/499: loss=5.315188759106016, w0=72.83124999999993, w1=16.149103062754282\n",
      "SubSGD iter. 499/499: loss=5.31581633319646, w0=72.77499999999992, w1=16.15996220365184\n",
      "SubSGD: execution time=0.522 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.3\n",
    "batch_size = 32\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([1, 1])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2bb557f0964df6beb6512571543b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
